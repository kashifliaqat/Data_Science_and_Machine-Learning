{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CartPole Problem\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kashifliaqat/Data_Science_and_Machine-Learning/blob/main/Reinforcement_Learning/CartPole_Problem.ipynb)\n",
    "\n",
    "The classic CartPole problem is a control problem in which the goal is to balance a pole on a cart moving along a horizontal track. The problem is considered solved when the pole is balanced for a certain duration without falling over.\n",
    "\n",
    "- The implementation of Reinforcement Learning (RL) in the code involves training an RL algorithm to learn a policy that can balance the pole on the cart. Specifically, the Proximal Policy Optimization (PPO) algorithm is used to learn the policy. PPO is a model-free RL algorithm that uses a policy gradient method to update the policy.\n",
    "\n",
    "- The code first creates an instance of the CartPole environment using the OpenAI Gym library. It then runs 10 episodes of the environment, where each episode involves repeatedly selecting random actions and observing the resulting rewards until the pole falls or the maximum number of steps is reached. This is to get a baseline performance of the environment.\n",
    "\n",
    "- Next, a PPO model is created using the stable_baselines3 library. The model is trained on the CartPole environment for a specified number of timesteps. After training, the trained model is used to evaluate the policy on the environment for 10 episodes, with rendering turned on to visualize the performance of the policy.\n",
    "\n",
    "- Finally, the trained model is used to run 10 episodes of the CartPole environment, where each episode involves selecting an action using the trained model's policy and observing the resulting rewards until the pole falls or the maximum number of steps is reached. The scores for each episode are printed to the console.\n",
    "\n",
    "Overall, the implementation of RL in the code involves using a PPO algorithm to learn a policy that can balance the pole on the cart in the CartPole environment. The code demonstrates the basic steps involved in training and evaluating an RL algorithm, including creating an environment, creating a model, training the model, and evaluating the policy on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment name and create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to run episodes of the environment\n",
    "10 episodes of the environment are run and at each time step randomly selects an action from the action space, until the episode is completed. It then prints the score achieved in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 22.0\n",
      "Episode: 2 Score: 14.0\n",
      "Episode: 3 Score: 31.0\n",
      "Episode: 4 Score: 28.0\n",
      "Episode: 5 Score: 16.0\n",
      "Episode: 6 Score: 12.0\n",
      "Episode: 7 Score: 38.0\n",
      "Episode: 8 Score: 15.0\n",
      "Episode: 9 Score: 16.0\n",
      "Episode: 10 Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, 11):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # loop until episode is completed\n",
    "    while not done:\n",
    "        env.render() # render the environment\n",
    "        action = env.action_space.sample() # randomly select an action\n",
    "        n_state, reward, done, info = env.step(action) # take the action and observe the next state, reward and done flag\n",
    "        score += reward # add the reward to the score\n",
    "        \n",
    "    # print episode number and score\n",
    "    print('Episode:', episode, 'Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # close the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Create a vectorized environment from the previously created environment, creates an instance of the PPO model, specifies the policy to be used ('MlpPolicy'), and trains the model for 20000 timesteps. Finally, it saves the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 772  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008211978 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00479     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.09        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 52.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009248704 |\n",
      "|    clip_fraction        | 0.0638      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0568      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009436092 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 46.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 489         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008667408 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 61.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073356144 |\n",
      "|    clip_fraction        | 0.0844       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0164      |\n",
      "|    value_loss           | 58.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009921404 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005123173 |\n",
      "|    clip_fraction        | 0.0395      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.78        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00926    |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 467          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037419905 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | 0.319        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.7         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 76.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009523734 |\n",
      "|    clip_fraction        | 0.0632      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    value_loss           | 29.5        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name) # create a new environment\n",
    "env = DummyVecEnv([lambda: env]) # create a vectorized environment from the environment\n",
    "model = PPO('MlpPolicy', env, verbose=1) # create a PPO model with MLP policy and vectorized environment\n",
    "model.learn(total_timesteps=20000) # train the model for 20000 timesteps\n",
    "\n",
    "# save the model\n",
    "model.save('ppo model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "Evaluate the trained model on the environment for 10 episodes and displays the results, with render set to True to visualize the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the trained model\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to run episodes of the environment using the trained model\n",
    "Run 10 episodes of the environment using the trained model to select actions at each time step. It then prints the score achieved in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: [200.]\n",
      "Episode: 2 Score: [200.]\n",
      "Episode: 3 Score: [200.]\n",
      "Episode: 4 Score: [200.]\n",
      "Episode: 5 Score: [200.]\n",
      "Episode: 6 Score: [200.]\n",
      "Episode: 7 Score: [200.]\n",
      "Episode: 8 Score: [200.]\n",
      "Episode: 9 Score: [200.]\n",
      "Episode: 10 Score: [200.]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, 11):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # loop until episode is completed\n",
    "    while not done:\n",
    "        env.render() # render the environment\n",
    "        action, _ = model.predict(obs) # use the trained model to select an action\n",
    "        obs, reward, done, info = env.step(action) # take the action and observe the next state, reward and done flag\n",
    "        score += reward # add the reward to the score\n",
    "        \n",
    "    # print episode number and score\n",
    "    print('Episode:', episode, 'Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # close the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "##### Before Training\n",
    "<p align=\"center\"><img src=\"https://github.com/kashifliaqat/Data_Science_and_Machine-Learning/raw/main/Images/cart_1.gif\" alt=\"CartPool Before Training\" width=\"500\" height=\"300\">\n",
    "\n",
    "##### After Training\n",
    "<p align=\"center\"><img src=\"https://github.com/kashifliaqat/Data_Science_and_Machine-Learning/raw/main/Images/cart_2.gif\" alt=\"CartPool After Training\" width=\"500\" height=\"300\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
