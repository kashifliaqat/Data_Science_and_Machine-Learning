{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impabalanced Data Problem\n",
    "Imbalanced data is a common problem in machine learning. It occurs when the number of observations in each class is not the same. This can be a problem because most machine learning algorithms are designed to maximize overall accuracy and can result in models that have poor predictive performance, even on the majority class. In this notebook, we will explore how to handle imbalanced data using the healthcare stroke dataset.\n",
    "\n",
    "### Dataset\n",
    "The dataset contains 5110 observations and 12 features. The “stroke” column is the target variable. The dataset is imbalanced, where the majority class (no stroke) accounts for 95.13% of the observations and the minority class (stroke) accounts for 4.87% of the observations.\n",
    "\n",
    "### Objective\n",
    "The objective of this notebook is to build a machine learning model to predict whether a patient will have a stroke or not. The specific focus of this notebook is to handle the imbalanced data problem.\n",
    "\n",
    "### Approach\n",
    "We will use the following approach to handle the imbalanced data problem:\n",
    "1.    Load and visualize the dataset\n",
    "2.    Verify the imbalance in the dataset\n",
    "3.    Use techniques to handle the imbalance in the dataset (SMOTE, and undersampling)\n",
    "4.    Build multiple machine learning models using GridSearchCV to find the best hyperparameters\n",
    "5.    Evaluate the performance of the models using the test set\n",
    "6.    Compare the performance of the models and data Preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from local directory\n",
    "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data information\n",
    "print(\"Data Information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "# Drop null values\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class count\n",
    "class_count = df['stroke'].value_counts()\n",
    "print(\"Class Count:\")\n",
    "print(class_count)\n",
    "\n",
    "# Visualize class count\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='stroke', data=df)\n",
    "plt.title(\"Class Count\")\n",
    "plt.xlabel(\"Stroke\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Check for imbalanced data\n",
    "imbalanced = class_count[0] / class_count[1]\n",
    "print(\"\\nImbalanced Data Ratio (Non-Stroke:Stroke): {:.2f}\".format(imbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle('Healthcare Dataset for Stroke Prediction', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Age Distribution Plot\n",
    "sns.histplot(df[df['stroke'] == 0]['age'], bins=30, color='blue', label='Non-Stroke', ax=axes[0, 0])\n",
    "sns.histplot(df[df['stroke'] == 1]['age'], bins=30, color='red', label='Stroke', ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Age Distribution\")\n",
    "axes[0, 0].set_xlabel(\"Age\")\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Gender Count Plot\n",
    "sns.countplot(x='gender', data=df, hue='stroke', ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Gender Count\")\n",
    "axes[0, 1].set_xlabel(\"Gender\")\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "axes[0, 1].legend(title='Stroke', labels=['No', 'Yes'])\n",
    "\n",
    "# Hypertension Count Plot\n",
    "sns.countplot(x='hypertension', data=df, hue='stroke', ax=axes[0, 2])\n",
    "axes[0, 2].set_title(\"Hypertension Count\")\n",
    "axes[0, 2].set_xlabel(\"Hypertension\")\n",
    "axes[0, 2].set_ylabel(\"Count\")\n",
    "axes[0, 2].legend(title='Stroke', labels=['No', 'Yes'])\n",
    "\n",
    "# Heart Disease Count Plot\n",
    "sns.countplot(x='heart_disease', data=df, hue='stroke', ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Heart Disease Count\")\n",
    "axes[1, 0].set_xlabel(\"Heart Disease\")\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "axes[1, 0].legend(title='Stroke', labels=['No', 'Yes'])\n",
    "\n",
    "# Marital Status Count Plot\n",
    "sns.countplot(x='ever_married', data=df, hue='stroke', ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Marital Status Count\")\n",
    "axes[1, 1].set_xlabel(\"Marital Status\")\n",
    "axes[1, 1].set_ylabel(\"Count\")\n",
    "axes[1, 1].legend(title='Stroke', labels=['No', 'Yes'])\n",
    "\n",
    "# BMI Distribution Plot\n",
    "sns.histplot(df[df['stroke'] == 0]['bmi'].dropna(), bins=30, color='blue', label='Non-Stroke', ax=axes[1, 2])\n",
    "sns.histplot(df[df['stroke'] == 1]['bmi'].dropna(), bins=30, color='red', label='Stroke', ax=axes[1, 2])\n",
    "axes[1, 2].set_title(\"BMI Distribution\")\n",
    "axes[1, 2].set_xlabel(\"BMI\")\n",
    "axes[1, 2].set_ylabel(\"Count\")\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the categorical columns\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "# Apply label encoding to the \"gender\" column\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['ever_married'] = le.fit_transform(df['ever_married'])\n",
    "df['work_type'] = le.fit_transform(df['work_type'])\n",
    "df['smoking_status'] = le.fit_transform(df['smoking_status'])\n",
    "df['Residence_type'] = le.fit_transform(df['Residence_type'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Dataframes\n",
    "1. Complete Dataset with categorical and numerical features. The Categorical features are encoded using LabelEncoder.\n",
    "2. Dataset with only numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe with numeric columns\n",
    "df_num = df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'stroke']].dropna()\n",
    "# Separate the features and the target for numeric data\n",
    "X_num, y_num = df_num[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']], df_num['stroke']\n",
    "# Separate the features and target variable for complete data with categorical columns encoded\n",
    "X = df.drop('stroke', axis=1)\n",
    "y = df['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Imbalanced Data Problem\n",
    "We will use the following methods to handle the imbalanced data problem: SMOTE, and . \n",
    "- SMOTE is oversampling technique\n",
    "- Random Undersampling is undersampling technique\n",
    "\n",
    "#### SMOTE\n",
    "Synthetic Minority Oversampling Technique (SMOTE) is an oversampling technique that creates synthetic samples from the minority class. It works by selecting examples that are close in the feature space, drawing a line between a randomly chosen pair of examples and drawing a new sample at a point along that line.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/kashifliaqat/Data_Science_and_Machine-Learning/raw/main/Images/SMOTE.PNG\" alt=\"Synthetic Minority Oversampling Technique (SMOTE)\" width=\"600\" height=\"250\">\n",
    "\n",
    "#### Random Undersampling \n",
    "Random undersampling is an undersampling technique that randomly removes observations from the majority class to prevent its signal from dominating the learning algorithm.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/kashifliaqat/Data_Science_and_Machine-Learning/raw/main/Images/SMOTE.PNG\" alt=\"Random Undersampling\" width=\"600\" height=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the class count for the original data\n",
    "sns.countplot(x=y, ax=axes[0])\n",
    "axes[0].set_title('Original Data')\n",
    "print(\"Original Data:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Plot the class count for the SMOTE data\n",
    "sns.countplot(x=y_smote, ax=axes[1])\n",
    "axes[1].set_title('SMOTE')\n",
    "print(\"\\nSMOTE:\")\n",
    "print(y_smote.value_counts())\n",
    "\n",
    "# Apply random undersampling\n",
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "\n",
    "# Plot the class count for the random undersampling data\n",
    "sns.countplot(x=y_rus, ax=axes[2])\n",
    "axes[2].set_title('Random Undersampling')\n",
    "print(\"\\nRandom Undersampling:\")\n",
    "print(y_rus.value_counts())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_num, y_num, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the classifiers and their parameter grids for GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10, 15], 'min_samples_split': [2, 5, 10],'criterion': ['gini', 'entropy']}\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt_params = {'max_depth': [None, 5, 10, 15, 20], 'min_samples_split': [2, 5, 10], 'criterion': ['gini', 'entropy']}\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr_params = {'C': [0.1, 1, 10]}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {'n_neighbors': [3, 5, 7, 9]}\n",
    "\n",
    "# Define the data techniques\n",
    "oversampler = SMOTE(random_state=42)\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# List to store the results\n",
    "store_results = []\n",
    "store_accuracies = []\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Loop through different data techniques\n",
    "for i, (name, sampler) in enumerate({'Original Data': None, 'SMOTE Data': oversampler, 'Undersampled Data': undersampler}.items()):\n",
    "    if sampler is not None:\n",
    "        X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    \n",
    "    # Loop through different classifiers\n",
    "    for j, (clf_name, clf_params) in enumerate({'Random Forest': (rf, rf_params), 'Decision Tree': (dt, dt_params),\n",
    "                                                'Logistic Regression': (lr, lr_params), 'KNN': (knn, knn_params)}.items()):\n",
    "        clf = clf_params[0]\n",
    "        params = clf_params[1]\n",
    "        # Perform GridSearchCV with 5-fold cross validation\n",
    "        grid_clf = GridSearchCV(clf, params, cv=5)\n",
    "        grid_clf.fit(X_train_res, y_train_res)\n",
    "        best_clf = grid_clf.best_estimator_\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        y_pred = best_clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        classification_rep = classification_report(y_test, y_pred)\n",
    "        \n",
    "        # Store the results\n",
    "        store_results.append({'Data Technique': name, 'Classifier': clf_name, 'Best Params': grid_clf.best_params_,\n",
    "                        'Validation Accuracy': grid_clf.best_score_, 'Test Accuracy': accuracy, 'Classification Report': classification_rep})\n",
    "        # Store the accuracies only\n",
    "        store_accuracies.append({'Data Technique': name, 'Classifier': clf_name, 'Validation Accuracy': grid_clf.best_score_, 'Test Accuracy': accuracy})\n",
    "\n",
    "        print(f\"Finished {clf_name} with {name}\")\n",
    "        print('Test Accuracy', accuracy)\n",
    "# Convert results to dataframe\n",
    "results_df = pd.DataFrame(store_results)\n",
    "accuracies_df = pd.DataFrame(store_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(accuracies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group the results by Data Technique and Classifier, and calculate the mean Test Accuracy for each group\n",
    "grouped_results = accuracies_df.groupby(['Data Technique', 'Classifier']).mean().reset_index()\n",
    "\n",
    "# Filter the necessary columns for the bar chart\n",
    "bar_chart_data = grouped_results[['Data Technique', 'Classifier', 'Test Accuracy']]\n",
    "\n",
    "# Pivot the data to have Data Technique as the index, Classifier as the columns, and Test Accuracy as the values\n",
    "bar_chart_data = bar_chart_data.pivot(index='Data Technique', columns='Classifier', values='Test Accuracy')\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the bar chart\n",
    "bar_chart_data.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Testing Accuracy by Data Technique and Classifier')\n",
    "plt.xlabel('Data Technique')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
